{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f846ab-0754-4e40-8912-3aa9b19edec9",
   "metadata": {},
   "source": [
    "# Testing StarDist network for instance segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c606305e-397e-47b2-8f36-6064372de8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals, absolute_import, division\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"image.interpolation\"] = 'none'\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import skimage as sk\n",
    "from tifffile import imread\n",
    "from csbdeep.utils import Path, normalize\n",
    "import napari\n",
    "import os\n",
    "from stardist import fill_label_holes, random_label_cmap, calculate_extents, gputools_available\n",
    "from stardist.matching import matching, matching_dataset\n",
    "from stardist.models import Config2D, StarDist2D, StarDistData2D\n",
    "\n",
    "np.random.seed(42)\n",
    "lbl_cmap = random_label_cmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fdca24-f2aa-47f5-87ea-77c3c838a3d7",
   "metadata": {},
   "source": [
    "Use the following 2 cells to convert pngs of masks exported from QuPath to Tiffs, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf81ea-69dc-47ca-ad8f-0a2dda1cf71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PNGs = sorted(glob('E:/Grainger_Lab/Amber/OIC-74_Zebrafish_RBC_Classification/tiles/**/instTiles/*.png',recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa2288e-2e16-465b-a865-323216a5879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converted all pngs to tiffs\n",
    "for y in PNGs:\n",
    "    path = os.path.dirname(y)\n",
    "    name = os.path.basename(y)\n",
    "    img = sk.io.imread(y)\n",
    "    sk.io.imsave(os.path.join(path,name[:-4]+'.tif'),img,check_contrast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6158c837-b51e-4d73-85aa-e4b255f7cb16",
   "metadata": {},
   "source": [
    "Read in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e19a90a3-d5de-42d4-9b85-21406cdac74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sorted(glob('E:/Grainger_Lab/Amber/OIC-74_Zebrafish_RBC_Classification/tiles/2025_01_28__0346-Scene-1-ScanRegion0/imgTiles/*.tif',recursive=True))\n",
    "Y = sorted(glob('E:/Grainger_Lab/Amber/OIC-74_Zebrafish_RBC_Classification/tiles/2025_01_28__0346-Scene-1-ScanRegion0/instTiles/*.tif',recursive=True))\n",
    "assert all(Path(x).name==Path(y).name for x,y in zip(X,Y)) #added indexing here because the file type is different between the images and the masks, using indexing to match the name of the file without file type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7283c253-6558-4acf-a171-cd341b08e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(map(imread,X))\n",
    "Y = list(map(imread,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1c989de9-ca00-4c4e-9aac-923c80634ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channel = 1 if X[0].ndim == 2 else X[0].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "46dc5bb5-91ac-4b27-889e-6bde7ebc1b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Check number of channels\n",
    "print(n_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285a9ad4-1c43-4d46-b1a4-bece53309706",
   "metadata": {},
   "source": [
    "Normalize the images, fill possible holes in labels then split into train and validate groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "373b2f1c-785f-4fe1-9e96-ae0fbe58b5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing image channels independently.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 156/156 [00:13<00:00, 11.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 156/156 [00:04<00:00, 34.87it/s]\n"
     ]
    }
   ],
   "source": [
    "axis_norm = (0,1)   # normalize channels independently\n",
    "# axis_norm = (0,1,2) # normalize channels jointly\n",
    "if n_channel > 1:\n",
    "    print(\"Normalizing image channels %s.\" % ('jointly' if axis_norm is None or 2 in axis_norm else 'independently'))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "X = [normalize(x,1,99.8,axis=axis_norm) for x in tqdm(X)]\n",
    "Y = [fill_label_holes(y) for y in tqdm(Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f26a67bd-c58c-4ced-ba72-e1566bbd19da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images: 156\n",
      "- training:       133\n",
      "- validation:      23\n"
     ]
    }
   ],
   "source": [
    "assert len(X) > 1, \"not enough training data\"\n",
    "rng = np.random.RandomState(42)\n",
    "ind = rng.permutation(len(X))\n",
    "n_val = max(1, int(round(0.15 * len(ind))))\n",
    "ind_train, ind_val = ind[:-n_val], ind[-n_val:]\n",
    "X_val, Y_val = [X[i] for i in ind_val]  , [Y[i] for i in ind_val]\n",
    "X_trn, Y_trn = [X[i] for i in ind_train], [Y[i] for i in ind_train] \n",
    "print('number of images: %3d' % len(X))\n",
    "print('- training:       %3d' % len(X_trn))\n",
    "print('- validation:     %3d' % len(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6119b04-56d7-4424-abbd-b6aa80bad409",
   "metadata": {},
   "source": [
    "Set up hyperparameters for StarDist Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "318e8e18-5a33-49ad-a7af-745da2334853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_dim': 2,\n",
       " 'axes': 'YXC',\n",
       " 'n_channel_in': 3,\n",
       " 'n_channel_out': 33,\n",
       " 'train_checkpoint': 'weights_best.h5',\n",
       " 'train_checkpoint_last': 'weights_last.h5',\n",
       " 'train_checkpoint_epoch': 'weights_now.h5',\n",
       " 'n_rays': 32,\n",
       " 'grid': (4, 4),\n",
       " 'backbone': 'unet',\n",
       " 'n_classes': None,\n",
       " 'unet_n_depth': 3,\n",
       " 'unet_kernel_size': (3, 3),\n",
       " 'unet_n_filter_base': 32,\n",
       " 'unet_n_conv_per_depth': 2,\n",
       " 'unet_pool': (2, 2),\n",
       " 'unet_activation': 'relu',\n",
       " 'unet_last_activation': 'relu',\n",
       " 'unet_batch_norm': False,\n",
       " 'unet_dropout': 0.0,\n",
       " 'unet_prefix': '',\n",
       " 'net_conv_after_unet': 128,\n",
       " 'net_input_shape': (None, None, 3),\n",
       " 'net_mask_shape': (None, None, 1),\n",
       " 'train_shape_completion': False,\n",
       " 'train_completion_crop': 32,\n",
       " 'train_patch_size': (512, 512),\n",
       " 'train_background_reg': 0.0001,\n",
       " 'train_foreground_only': 0.9,\n",
       " 'train_sample_cache': True,\n",
       " 'train_dist_loss': 'mae',\n",
       " 'train_loss_weights': (1, 0.2),\n",
       " 'train_class_weights': (1, 1),\n",
       " 'train_epochs': 400,\n",
       " 'train_steps_per_epoch': 100,\n",
       " 'train_learning_rate': 0.0003,\n",
       " 'train_batch_size': 4,\n",
       " 'train_n_val_patches': None,\n",
       " 'train_tensorboard': True,\n",
       " 'train_reduce_lr': {'factor': 0.5, 'patience': 40, 'min_delta': 0},\n",
       " 'use_gpu': False}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 32 is a good default choice (see 1_data.ipynb)\n",
    "n_rays = 32\n",
    "\n",
    "# Use OpenCL-based computations for data generator during training (requires 'gputools')\n",
    "use_gpu = True and gputools_available()\n",
    "\n",
    "# Predict on subsampled grid for increased efficiency and larger field of view\n",
    "grid = (4,4)\n",
    "\n",
    "conf = Config2D (\n",
    "    n_rays       = n_rays,\n",
    "    grid         = grid,\n",
    "    use_gpu      = use_gpu,\n",
    "    n_channel_in = n_channel,\n",
    "    train_patch_size = (512,512),\n",
    "    train_steps_per_epoch = 100,\n",
    "    train_epochs = 400,\n",
    ")\n",
    "#print(conf)\n",
    "vars(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e50e69c-015f-4784-b78e-df5b98e389bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default values: prob_thresh=0.5, nms_thresh=0.4.\n"
     ]
    }
   ],
   "source": [
    "model = StarDist2D(conf, name='BloodCells', basedir='models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbccd8ad-5d81-45bf-a8c8-6b1de53c301f",
   "metadata": {},
   "source": [
    "Make sure that the field of view for the network is larger than the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c82c6900-f44e-419d-8864-3e4cb3d1f54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median object size:      [91. 91.]\n",
      "network field of view :  [189 189]\n"
     ]
    }
   ],
   "source": [
    "median_size = calculate_extents(list(Y), np.median)\n",
    "fov = np.array(model._axes_tile_overlap('YX'))\n",
    "print(f\"median object size:      {median_size}\")\n",
    "print(f\"network field of view :  {fov}\")\n",
    "# if any(median_size > fov):\n",
    "#     print(\"WARNING: median object size larger than field of view of the neural network.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac3fb3-0f39-4a78-a9b5-863cbfaa4a88",
   "metadata": {},
   "source": [
    "Define the augementations to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a83e6f1-2489-487b-b464-7b9f8f2c8422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_fliprot(img, mask): \n",
    "    assert img.ndim >= mask.ndim\n",
    "    axes = tuple(range(mask.ndim))\n",
    "    perm = tuple(np.random.permutation(axes))\n",
    "    img = img.transpose(perm + tuple(range(mask.ndim, img.ndim))) \n",
    "    mask = mask.transpose(perm) \n",
    "    for ax in axes: \n",
    "        if np.random.rand() > 0.5:\n",
    "            img = np.flip(img, axis=ax)\n",
    "            mask = np.flip(mask, axis=ax)\n",
    "    return img, mask \n",
    "\n",
    "def random_intensity_change(img):\n",
    "    img = img*np.random.uniform(0.6,2) + np.random.uniform(-0.2,0.2)\n",
    "    return img\n",
    "\n",
    "\n",
    "def augmenter(x, y):\n",
    "    \"\"\"Augmentation of a single input/label image pair.\n",
    "    x is an input image\n",
    "    y is the corresponding ground-truth label image\n",
    "    \"\"\"\n",
    "    x, y = random_fliprot(x, y)\n",
    "    x = random_intensity_change(x)\n",
    "    # add some gaussian noise\n",
    "    sig = 0.02*np.random.uniform(0,1)\n",
    "    x = x + sig*np.random.normal(0,1,x.shape)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ab663d-4184-47e8-b94e-0ef772383276",
   "metadata": {},
   "source": [
    "Train the model\n",
    "\n",
    "Use `tensorboard --logdir=.` in the command line in the same parent directory as the models (with StarDist env active) to watch live read out of training (could also put in the directory of the parent folder instead of cd to parent directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a95d3c59-f6f5-4b30-b509-08c2c5ed7ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628ms/step - dist_dist_iou_metric: 0.0721 - dist_loss: 33.3567 - dist_relevant_mae: 33.3558 - dist_relevant_mse: 1581.2866 - loss: 7.2958 - prob_kld: 0.4284 - prob_loss: 0.6245WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000178904ED1C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow | 5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000178904ED1C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 655ms/step - dist_dist_iou_metric: 0.0731 - dist_loss: 33.2973 - dist_relevant_mae: 33.2965 - dist_relevant_mse: 1577.1763 - loss: 7.2838 - prob_kld: 0.4282 - prob_loss: 0.6243 - val_dist_dist_iou_metric: 0.1664 - val_dist_loss: 22.7566 - val_dist_relevant_mae: 22.7244 - val_dist_relevant_mse: 840.6803 - val_loss: 4.8862 - val_prob_kld: 0.1570 - val_prob_loss: 0.3435 - learning_rate: 3.0000e-04\n",
      "Epoch 2/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 618ms/step - dist_dist_iou_metric: 0.3404 - dist_loss: 18.9244 - dist_relevant_mae: 18.9213 - dist_relevant_mse: 589.6682 - loss: 4.1632 - prob_kld: 0.1697 - prob_loss: 0.3783 - val_dist_dist_iou_metric: 0.3610 - val_dist_loss: 16.2768 - val_dist_relevant_mae: 16.2552 - val_dist_relevant_mse: 474.5856 - val_loss: 3.8963 - val_prob_kld: 0.4597 - val_prob_loss: 0.6462 - learning_rate: 3.0000e-04\n",
      "Epoch 3/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 605ms/step - dist_dist_iou_metric: 0.4524 - dist_loss: 14.3511 - dist_relevant_mae: 14.3488 - dist_relevant_mse: 377.7235 - loss: 3.2375 - prob_kld: 0.1622 - prob_loss: 0.3673 - val_dist_dist_iou_metric: 0.5995 - val_dist_loss: 9.5216 - val_dist_relevant_mae: 9.5362 - val_dist_relevant_mse: 185.0359 - val_loss: 2.2976 - val_prob_kld: 0.2074 - val_prob_loss: 0.3940 - learning_rate: 3.0000e-04\n",
      "Epoch 4/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 602ms/step - dist_dist_iou_metric: 0.5969 - dist_loss: 9.7120 - dist_relevant_mae: 9.7100 - dist_relevant_mse: 194.2020 - loss: 2.2254 - prob_kld: 0.0805 - prob_loss: 0.2830 - val_dist_dist_iou_metric: 0.6462 - val_dist_loss: 8.0608 - val_dist_relevant_mae: 8.0775 - val_dist_relevant_mse: 141.4191 - val_loss: 1.8329 - val_prob_kld: 0.0327 - val_prob_loss: 0.2192 - learning_rate: 3.0000e-04\n",
      "Epoch 5/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 608ms/step - dist_dist_iou_metric: 0.6483 - dist_loss: 8.1699 - dist_relevant_mae: 8.1680 - dist_relevant_mse: 142.9079 - loss: 1.8679 - prob_kld: 0.0321 - prob_loss: 0.2339 - val_dist_dist_iou_metric: 0.6493 - val_dist_loss: 7.4998 - val_dist_relevant_mae: 7.5092 - val_dist_relevant_mse: 133.4684 - val_loss: 1.7198 - val_prob_kld: 0.0330 - val_prob_loss: 0.2195 - learning_rate: 3.0000e-04\n",
      "Epoch 6/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 610ms/step - dist_dist_iou_metric: 0.6662 - dist_loss: 7.6305 - dist_relevant_mae: 7.6284 - dist_relevant_mse: 129.4458 - loss: 1.7656 - prob_kld: 0.0347 - prob_loss: 0.2395 - val_dist_dist_iou_metric: 0.7170 - val_dist_loss: 6.2174 - val_dist_relevant_mae: 6.2354 - val_dist_relevant_mse: 93.9362 - val_loss: 1.4660 - val_prob_kld: 0.0343 - val_prob_loss: 0.2208 - learning_rate: 3.0000e-04\n",
      "Epoch 7/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 610ms/step - dist_dist_iou_metric: 0.7039 - dist_loss: 6.6241 - dist_relevant_mae: 6.6221 - dist_relevant_mse: 105.7842 - loss: 1.5606 - prob_kld: 0.0319 - prob_loss: 0.2358 - val_dist_dist_iou_metric: 0.7562 - val_dist_loss: 5.4812 - val_dist_relevant_mae: 5.5025 - val_dist_relevant_mse: 74.9566 - val_loss: 1.3120 - val_prob_kld: 0.0267 - val_prob_loss: 0.2132 - learning_rate: 3.0000e-04\n",
      "Epoch 8/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 607ms/step - dist_dist_iou_metric: 0.7379 - dist_loss: 5.8535 - dist_relevant_mae: 5.8515 - dist_relevant_mse: 88.5622 - loss: 1.4037 - prob_kld: 0.0284 - prob_loss: 0.2330 - val_dist_dist_iou_metric: 0.7676 - val_dist_loss: 5.3156 - val_dist_relevant_mae: 5.3368 - val_dist_relevant_mse: 67.5487 - val_loss: 1.2726 - val_prob_kld: 0.0205 - val_prob_loss: 0.2070 - learning_rate: 3.0000e-04\n",
      "Epoch 9/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 604ms/step - dist_dist_iou_metric: 0.7624 - dist_loss: 5.2676 - dist_relevant_mae: 5.2657 - dist_relevant_mse: 75.1181 - loss: 1.2840 - prob_kld: 0.0263 - prob_loss: 0.2305 - val_dist_dist_iou_metric: 0.7927 - val_dist_loss: 4.5388 - val_dist_relevant_mae: 4.5590 - val_dist_relevant_mse: 57.4421 - val_loss: 1.1152 - val_prob_kld: 0.0187 - val_prob_loss: 0.2053 - learning_rate: 3.0000e-04\n",
      "Epoch 10/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 614ms/step - dist_dist_iou_metric: 0.7808 - dist_loss: 4.8622 - dist_relevant_mae: 4.8604 - dist_relevant_mse: 66.2403 - loss: 1.1958 - prob_kld: 0.0235 - prob_loss: 0.2234 - val_dist_dist_iou_metric: 0.8131 - val_dist_loss: 3.9676 - val_dist_relevant_mae: 3.9877 - val_dist_relevant_mse: 48.8470 - val_loss: 1.0063 - val_prob_kld: 0.0240 - val_prob_loss: 0.2105 - learning_rate: 3.0000e-04\n",
      "Epoch 11/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 601ms/step - dist_dist_iou_metric: 0.7947 - dist_loss: 4.5547 - dist_relevant_mae: 4.5527 - dist_relevant_mse: 61.1460 - loss: 1.1368 - prob_kld: 0.0240 - prob_loss: 0.2258 - val_dist_dist_iou_metric: 0.8139 - val_dist_loss: 4.0581 - val_dist_relevant_mae: 4.0817 - val_dist_relevant_mse: 50.1820 - val_loss: 1.0197 - val_prob_kld: 0.0186 - val_prob_loss: 0.2051 - learning_rate: 3.0000e-04\n",
      "Epoch 12/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 611ms/step - dist_dist_iou_metric: 0.8018 - dist_loss: 4.3590 - dist_relevant_mae: 4.3571 - dist_relevant_mse: 54.7487 - loss: 1.0933 - prob_kld: 0.0207 - prob_loss: 0.2215 - val_dist_dist_iou_metric: 0.8203 - val_dist_loss: 3.8035 - val_dist_relevant_mae: 3.8207 - val_dist_relevant_mse: 47.0094 - val_loss: 0.9672 - val_prob_kld: 0.0184 - val_prob_loss: 0.2049 - learning_rate: 3.0000e-04\n",
      "Epoch 13/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 613ms/step - dist_dist_iou_metric: 0.8086 - dist_loss: 4.2382 - dist_relevant_mae: 4.2361 - dist_relevant_mse: 55.5000 - loss: 1.0750 - prob_kld: 0.0258 - prob_loss: 0.2274 - val_dist_dist_iou_metric: 0.8228 - val_dist_loss: 3.6953 - val_dist_relevant_mae: 3.7157 - val_dist_relevant_mse: 45.5559 - val_loss: 0.9445 - val_prob_kld: 0.0167 - val_prob_loss: 0.2032 - learning_rate: 3.0000e-04\n",
      "Epoch 14/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 594ms/step - dist_dist_iou_metric: 0.8080 - dist_loss: 4.2532 - dist_relevant_mae: 4.2513 - dist_relevant_mse: 55.3728 - loss: 1.0812 - prob_kld: 0.0206 - prob_loss: 0.2306 - val_dist_dist_iou_metric: 0.8313 - val_dist_loss: 3.6589 - val_dist_relevant_mae: 3.6778 - val_dist_relevant_mse: 40.2813 - val_loss: 0.9358 - val_prob_kld: 0.0154 - val_prob_loss: 0.2019 - learning_rate: 3.0000e-04\n",
      "Epoch 15/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 596ms/step - dist_dist_iou_metric: 0.8183 - dist_loss: 3.9850 - dist_relevant_mae: 3.9830 - dist_relevant_mse: 49.5285 - loss: 1.0218 - prob_kld: 0.0249 - prob_loss: 0.2248 - val_dist_dist_iou_metric: 0.8442 - val_dist_loss: 3.3511 - val_dist_relevant_mae: 3.3693 - val_dist_relevant_mse: 37.5541 - val_loss: 0.8751 - val_prob_kld: 0.0165 - val_prob_loss: 0.2030 - learning_rate: 3.0000e-04\n",
      "Epoch 16/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 595ms/step - dist_dist_iou_metric: 0.8217 - dist_loss: 3.8990 - dist_relevant_mae: 3.8970 - dist_relevant_mse: 47.6829 - loss: 1.0025 - prob_kld: 0.0184 - prob_loss: 0.2227 - val_dist_dist_iou_metric: 0.8403 - val_dist_loss: 3.3981 - val_dist_relevant_mae: 3.4146 - val_dist_relevant_mse: 39.5079 - val_loss: 0.8851 - val_prob_kld: 0.0175 - val_prob_loss: 0.2040 - learning_rate: 3.0000e-04\n",
      "Epoch 17/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 600ms/step - dist_dist_iou_metric: 0.8338 - dist_loss: 3.6071 - dist_relevant_mae: 3.6051 - dist_relevant_mse: 43.4627 - loss: 0.9452 - prob_kld: 0.0208 - prob_loss: 0.2237 - val_dist_dist_iou_metric: 0.8522 - val_dist_loss: 3.2063 - val_dist_relevant_mae: 3.2263 - val_dist_relevant_mse: 36.6697 - val_loss: 0.8474 - val_prob_kld: 0.0173 - val_prob_loss: 0.2038 - learning_rate: 3.0000e-04\n",
      "Epoch 18/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 605ms/step - dist_dist_iou_metric: 0.8335 - dist_loss: 3.6518 - dist_relevant_mae: 3.6499 - dist_relevant_mse: 45.2426 - loss: 0.9530 - prob_kld: 0.0196 - prob_loss: 0.2226 - val_dist_dist_iou_metric: 0.8330 - val_dist_loss: 3.4016 - val_dist_relevant_mae: 3.4201 - val_dist_relevant_mse: 38.2047 - val_loss: 0.8831 - val_prob_kld: 0.0145 - val_prob_loss: 0.2010 - learning_rate: 3.0000e-04\n",
      "Epoch 19/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 598ms/step - dist_dist_iou_metric: 0.8316 - dist_loss: 3.6728 - dist_relevant_mae: 3.6709 - dist_relevant_mse: 42.9454 - loss: 0.9534 - prob_kld: 0.0180 - prob_loss: 0.2188 - val_dist_dist_iou_metric: 0.8526 - val_dist_loss: 3.1410 - val_dist_relevant_mae: 3.1626 - val_dist_relevant_mse: 34.2399 - val_loss: 0.8310 - val_prob_kld: 0.0138 - val_prob_loss: 0.2003 - learning_rate: 3.0000e-04\n",
      "Epoch 20/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 602ms/step - dist_dist_iou_metric: 0.8373 - dist_loss: 3.5517 - dist_relevant_mae: 3.5499 - dist_relevant_mse: 41.8886 - loss: 0.9345 - prob_kld: 0.0214 - prob_loss: 0.2242 - val_dist_dist_iou_metric: 0.8566 - val_dist_loss: 3.0597 - val_dist_relevant_mae: 3.0777 - val_dist_relevant_mse: 33.8102 - val_loss: 0.8136 - val_prob_kld: 0.0134 - val_prob_loss: 0.1999 - learning_rate: 3.0000e-04\n",
      "Epoch 21/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 596ms/step - dist_dist_iou_metric: 0.8491 - dist_loss: 3.2479 - dist_relevant_mae: 3.2461 - dist_relevant_mse: 34.6339 - loss: 0.8640 - prob_kld: 0.0156 - prob_loss: 0.2144 - val_dist_dist_iou_metric: 0.8582 - val_dist_loss: 2.9454 - val_dist_relevant_mae: 2.9625 - val_dist_relevant_mse: 30.2172 - val_loss: 0.7921 - val_prob_kld: 0.0149 - val_prob_loss: 0.2014 - learning_rate: 3.0000e-04\n",
      "Epoch 22/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 600ms/step - dist_dist_iou_metric: 0.8467 - dist_loss: 3.3062 - dist_relevant_mae: 3.3043 - dist_relevant_mse: 36.1219 - loss: 0.8785 - prob_kld: 0.0153 - prob_loss: 0.2173 - val_dist_dist_iou_metric: 0.8339 - val_dist_loss: 3.4471 - val_dist_relevant_mae: 3.4649 - val_dist_relevant_mse: 38.5241 - val_loss: 0.8909 - val_prob_kld: 0.0133 - val_prob_loss: 0.1998 - learning_rate: 3.0000e-04\n",
      "Epoch 23/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 609ms/step - dist_dist_iou_metric: 0.8359 - dist_loss: 3.5730 - dist_relevant_mae: 3.5712 - dist_relevant_mse: 38.9660 - loss: 0.9389 - prob_kld: 0.0198 - prob_loss: 0.2243 - val_dist_dist_iou_metric: 0.8667 - val_dist_loss: 2.8107 - val_dist_relevant_mae: 2.8284 - val_dist_relevant_mse: 29.7587 - val_loss: 0.7800 - val_prob_kld: 0.0300 - val_prob_loss: 0.2165 - learning_rate: 3.0000e-04\n",
      "Epoch 24/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 595ms/step - dist_dist_iou_metric: 0.8482 - dist_loss: 3.2679 - dist_relevant_mae: 3.2662 - dist_relevant_mse: 36.2282 - loss: 0.8776 - prob_kld: 0.0199 - prob_loss: 0.2241 - val_dist_dist_iou_metric: 0.8585 - val_dist_loss: 3.0964 - val_dist_relevant_mae: 3.1124 - val_dist_relevant_mse: 29.8329 - val_loss: 0.8219 - val_prob_kld: 0.0148 - val_prob_loss: 0.2014 - learning_rate: 3.0000e-04\n",
      "Epoch 25/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 606ms/step - dist_dist_iou_metric: 0.8458 - dist_loss: 3.3585 - dist_relevant_mae: 3.3567 - dist_relevant_mse: 36.3435 - loss: 0.8866 - prob_kld: 0.0152 - prob_loss: 0.2149 - val_dist_dist_iou_metric: 0.8462 - val_dist_loss: 3.4701 - val_dist_relevant_mae: 3.4808 - val_dist_relevant_mse: 30.0816 - val_loss: 0.8967 - val_prob_kld: 0.0159 - val_prob_loss: 0.2024 - learning_rate: 3.0000e-04\n",
      "Epoch 26/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 615ms/step - dist_dist_iou_metric: 0.8461 - dist_loss: 3.3620 - dist_relevant_mae: 3.3602 - dist_relevant_mse: 36.9064 - loss: 0.8957 - prob_kld: 0.0169 - prob_loss: 0.2234 - val_dist_dist_iou_metric: 0.8671 - val_dist_loss: 2.8404 - val_dist_relevant_mae: 2.8543 - val_dist_relevant_mse: 27.0825 - val_loss: 0.7671 - val_prob_kld: 0.0116 - val_prob_loss: 0.1981 - learning_rate: 3.0000e-04\n",
      "Epoch 27/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 654ms/step - dist_dist_iou_metric: 0.8597 - dist_loss: 3.0060 - dist_relevant_mae: 3.0043 - dist_relevant_mse: 31.4810 - loss: 0.8154 - prob_kld: 0.0141 - prob_loss: 0.2142 - val_dist_dist_iou_metric: 0.8738 - val_dist_loss: 2.6660 - val_dist_relevant_mae: 2.6848 - val_dist_relevant_mse: 27.0373 - val_loss: 0.7337 - val_prob_kld: 0.0121 - val_prob_loss: 0.1986 - learning_rate: 3.0000e-04\n",
      "Epoch 28/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 613ms/step - dist_dist_iou_metric: 0.8560 - dist_loss: 3.1031 - dist_relevant_mae: 3.1013 - dist_relevant_mse: 31.9492 - loss: 0.8370 - prob_kld: 0.0136 - prob_loss: 0.2164 - val_dist_dist_iou_metric: 0.8596 - val_dist_loss: 3.1260 - val_dist_relevant_mae: 3.1403 - val_dist_relevant_mse: 29.7283 - val_loss: 0.8240 - val_prob_kld: 0.0113 - val_prob_loss: 0.1978 - learning_rate: 3.0000e-04\n",
      "Epoch 29/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 611ms/step - dist_dist_iou_metric: 0.8547 - dist_loss: 3.1515 - dist_relevant_mae: 3.1497 - dist_relevant_mse: 35.1373 - loss: 0.8489 - prob_kld: 0.0143 - prob_loss: 0.2186 - val_dist_dist_iou_metric: 0.8748 - val_dist_loss: 2.6883 - val_dist_relevant_mae: 2.6996 - val_dist_relevant_mse: 24.6056 - val_loss: 0.7359 - val_prob_kld: 0.0112 - val_prob_loss: 0.1977 - learning_rate: 3.0000e-04\n",
      "Epoch 30/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 611ms/step - dist_dist_iou_metric: 0.8595 - dist_loss: 3.0183 - dist_relevant_mae: 3.0162 - dist_relevant_mse: 30.7661 - loss: 0.8184 - prob_kld: 0.0140 - prob_loss: 0.2147 - val_dist_dist_iou_metric: 0.8597 - val_dist_loss: 2.8972 - val_dist_relevant_mae: 2.9130 - val_dist_relevant_mse: 29.0410 - val_loss: 0.8158 - val_prob_kld: 0.0492 - val_prob_loss: 0.2357 - learning_rate: 3.0000e-04\n",
      "Epoch 31/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 610ms/step - dist_dist_iou_metric: 0.8616 - dist_loss: 3.0023 - dist_relevant_mae: 3.0004 - dist_relevant_mse: 31.1587 - loss: 0.8213 - prob_kld: 0.0182 - prob_loss: 0.2208 - val_dist_dist_iou_metric: 0.8601 - val_dist_loss: 2.9253 - val_dist_relevant_mae: 2.9394 - val_dist_relevant_mse: 27.6704 - val_loss: 0.7843 - val_prob_kld: 0.0117 - val_prob_loss: 0.1983 - learning_rate: 3.0000e-04\n",
      "Epoch 32/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 617ms/step - dist_dist_iou_metric: 0.8603 - dist_loss: 3.0204 - dist_relevant_mae: 3.0185 - dist_relevant_mse: 33.7815 - loss: 0.8240 - prob_kld: 0.0165 - prob_loss: 0.2199 - val_dist_dist_iou_metric: 0.8668 - val_dist_loss: 2.7277 - val_dist_relevant_mae: 2.7441 - val_dist_relevant_mse: 26.1979 - val_loss: 0.7460 - val_prob_kld: 0.0125 - val_prob_loss: 0.1990 - learning_rate: 3.0000e-04\n",
      "Epoch 33/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 618ms/step - dist_dist_iou_metric: 0.8655 - dist_loss: 2.8709 - dist_relevant_mae: 2.8691 - dist_relevant_mse: 29.5686 - loss: 0.7961 - prob_kld: 0.0135 - prob_loss: 0.2219 - val_dist_dist_iou_metric: 0.8740 - val_dist_loss: 2.6086 - val_dist_relevant_mae: 2.6216 - val_dist_relevant_mse: 24.9336 - val_loss: 0.7208 - val_prob_kld: 0.0119 - val_prob_loss: 0.1984 - learning_rate: 3.0000e-04\n",
      "Epoch 34/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 615ms/step - dist_dist_iou_metric: 0.8665 - dist_loss: 2.8597 - dist_relevant_mae: 2.8579 - dist_relevant_mse: 30.2161 - loss: 0.7836 - prob_kld: 0.0143 - prob_loss: 0.2116 - val_dist_dist_iou_metric: 0.8722 - val_dist_loss: 2.6066 - val_dist_relevant_mae: 2.6206 - val_dist_relevant_mse: 24.9445 - val_loss: 0.7222 - val_prob_kld: 0.0135 - val_prob_loss: 0.2000 - learning_rate: 3.0000e-04\n",
      "Epoch 35/400\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 625ms/step - dist_dist_iou_metric: 0.8641 - dist_loss: 2.9136 - dist_relevant_mae: 2.9119 - dist_relevant_mse: 31.1828 - loss: 0.7973 - prob_kld: 0.0141 - prob_loss: 0.2145 - val_dist_dist_iou_metric: 0.8794 - val_dist_loss: 2.5068 - val_dist_relevant_mae: 2.5214 - val_dist_relevant_mse: 24.3918 - val_loss: 0.7054 - val_prob_kld: 0.0167 - val_prob_loss: 0.2032 - learning_rate: 3.0000e-04\n",
      "Epoch 36/400\n",
      "\u001b[1m 84/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m9s\u001b[0m 599ms/step - dist_dist_iou_metric: 0.8727 - dist_loss: 2.7298 - dist_relevant_mae: 2.7280 - dist_relevant_mse: 27.5436 - loss: 0.7701 - prob_kld: 0.0165 - prob_loss: 0.2242 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(X_trn, Y_trn, validation_data\u001b[38;5;241m=\u001b[39m(X_val,Y_val), augmenter\u001b[38;5;241m=\u001b[39maugmenter)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\stardist\\Lib\\site-packages\\stardist\\models\\model2d.py:475\u001b[0m, in \u001b[0;36mStarDist2D.train\u001b[1;34m(self, X, Y, validation_data, classes, augmenter, seed, epochs, steps_per_epoch, workers)\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mappend(CARETensorBoardImage(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeras_model, data\u001b[38;5;241m=\u001b[39mdata_val, log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogdir\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    472\u001b[0m                                                    n_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, prob_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, output_slices\u001b[38;5;241m=\u001b[39moutput_slices))\n\u001b[0;32m    474\u001b[0m fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeras_model\u001b[38;5;241m.\u001b[39mfit_generator \u001b[38;5;28;01mif\u001b[39;00m (IS_TF_1 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m IS_KERAS_3_PLUS) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeras_model\u001b[38;5;241m.\u001b[39mfit\n\u001b[1;32m--> 475\u001b[0m history \u001b[38;5;241m=\u001b[39m fit(\u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_train), validation_data\u001b[38;5;241m=\u001b[39mdata_val,\n\u001b[0;32m    476\u001b[0m               epochs\u001b[38;5;241m=\u001b[39mepochs, steps_per_epoch\u001b[38;5;241m=\u001b[39msteps_per_epoch,\n\u001b[0;32m    477\u001b[0m               \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs,\n\u001b[0;32m    478\u001b[0m               callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    479\u001b[0m               \u001b[38;5;66;03m# set validation batchsize to training batchsize (only works for tf >= 2.2)\u001b[39;00m\n\u001b[0;32m    480\u001b[0m               \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mdict\u001b[39m(validation_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrain_batch_size) \u001b[38;5;28;01mif\u001b[39;00m _tf_version_at_least(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m {}))\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_finished()\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\stardist\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\stardist\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\stardist\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    218\u001b[0m     ):\n\u001b[1;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\stardist\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\stardist\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\stardist\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\stardist\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\stardist\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\stardist\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\stardist\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\stardist\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1689\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1690\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1691\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1692\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1693\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1694\u001b[0m   )\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\stardist\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(X_trn, Y_trn, validation_data=(X_val,Y_val), augmenter=augmenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce95d8-93a8-492f-9db2-a93fa151eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimize_thresholds(X_val, Y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StarDistGPU",
   "language": "python",
   "name": "stardistgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
