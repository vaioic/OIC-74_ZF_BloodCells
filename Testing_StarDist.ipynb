{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f846ab-0754-4e40-8912-3aa9b19edec9",
   "metadata": {},
   "source": [
    "# Testing StarDist network for instance segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c606305e-397e-47b2-8f36-6064372de8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals, absolute_import, division\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"image.interpolation\"] = 'none'\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import skimage as sk\n",
    "from tifffile import imread\n",
    "from csbdeep.utils import Path, normalize\n",
    "import napari\n",
    "import os\n",
    "from stardist import fill_label_holes, random_label_cmap, calculate_extents, gputools_available\n",
    "from stardist.matching import matching, matching_dataset\n",
    "from stardist.models import Config2D, StarDist2D, StarDistData2D\n",
    "\n",
    "np.random.seed(42)\n",
    "lbl_cmap = random_label_cmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6cf81ea-69dc-47ca-ad8f-0a2dda1cf71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PNGs = sorted(glob('E:/Grainger_Lab/Amber/OIC-74_Zebrafish_RBC_Classification/tiles/**/instTiles/*.png',recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "466338ad-ceff-43a5-b840-50100d9359b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12051"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(PNGs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaa2288e-2e16-465b-a865-323216a5879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converted all pngs to tiffs\n",
    "for y in PNGs:\n",
    "    path = os.path.dirname(y)\n",
    "    name = os.path.basename(y)\n",
    "    img = sk.io.imread(y)\n",
    "    sk.io.imsave(os.path.join(path,name[:-4]+'.tif'),img,check_contrast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19a90a3-d5de-42d4-9b85-21406cdac74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sorted(glob('E:/Grainger_Lab/Amber/OIC-74_Zebrafish_RBC_Classification/tiles/**/imgTiles/*.tif',recursive=True))\n",
    "Y = sorted(glob('E:/Grainger_Lab/Amber/OIC-74_Zebrafish_RBC_Classification/tiles/**/instTiles/*.tif',recursive=True))\n",
    "assert all(Path(x).name==Path(y).name for x,y in zip(X,Y)) #added indexing here because the file type is different between the images and the masks, using indexing to match the name of the file without file type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c600def7-9a91-4693-9ab7-e2fe6899442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y = sorted(glob('E:/Grainger_Lab/Amber/OIC-74_Zebrafish_RBC_Classification/tiles/2025_01_28__0346-Scene-1-ScanRegion0/instTiles/*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d520edd5-3fe1-4bb6-86ca-1f067facbbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.view_image(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7283c253-6558-4acf-a171-cd341b08e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(map(imread,X))\n",
    "Y = list(map(imread,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4542332e-9586-426f-a2e6-ee52837a1212",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [np.expand_dims(x, axis=0) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dc6ff11-58c9-40a4-bebd-560f6f36fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [np.expand_dims(y, axis=(0,-1)) for y in Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d102056c-0bf9-4c55-9899-4cf11dfd122a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c989de9-ca00-4c4e-9aac-923c80634ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channel = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdbea10d-b2c6-4cc2-af9f-540315ca7e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_labels = []\n",
    "for img in Y:\n",
    "    array = np.array(img)\n",
    "    array = (array * 255).astype(np.uint8)\n",
    "    Y_labels.append(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1361e1b5-9487-4d4c-bec9-04a831b1ebd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_labels[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b2f1c-785f-4fe1-9e96-ae0fbe58b5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing image channels independently.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▋                                                                     | 1198/12051 [01:34<13:43, 13.18it/s]"
     ]
    }
   ],
   "source": [
    "axis_norm = (0,1)   # normalize channels independently\n",
    "# axis_norm = (0,1,2) # normalize channels jointly\n",
    "if n_channel > 1:\n",
    "    print(\"Normalizing image channels %s.\" % ('jointly' if axis_norm is None or 2 in axis_norm else 'independently'))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "X = [normalize(x,1,99.8,axis=axis_norm) for x in tqdm(X)]\n",
    "Y = [fill_label_holes(y) for y in tqdm(Y_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c8417-1861-4421-9118-b4390a04a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f26a67bd-c58c-4ced-ba72-e1566bbd19da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images: 12051\n",
      "- training:       10243\n",
      "- validation:     1808\n"
     ]
    }
   ],
   "source": [
    "assert len(X) > 1, \"not enough training data\"\n",
    "rng = np.random.RandomState(42)\n",
    "ind = rng.permutation(len(X))\n",
    "n_val = max(1, int(round(0.15 * len(ind))))\n",
    "ind_train, ind_val = ind[:-n_val], ind[-n_val:]\n",
    "X_val, Y_val = [X[i] for i in ind_val]  , [Y_labels[i] for i in ind_val]\n",
    "X_trn, Y_trn = [X[i] for i in ind_train], [Y_labels[i] for i in ind_train] \n",
    "print('number of images: %3d' % len(X))\n",
    "print('- training:       %3d' % len(X_trn))\n",
    "print('- validation:     %3d' % len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "318e8e18-5a33-49ad-a7af-745da2334853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config2D(n_dim=2, axes='YXC', n_channel_in=3, n_channel_out=33, train_checkpoint='weights_best.h5', train_checkpoint_last='weights_last.h5', train_checkpoint_epoch='weights_now.h5', n_rays=32, grid=(4, 4), backbone='unet', n_classes=None, unet_n_depth=3, unet_kernel_size=(3, 3), unet_n_filter_base=32, unet_n_conv_per_depth=2, unet_pool=(2, 2), unet_activation='relu', unet_last_activation='relu', unet_batch_norm=False, unet_dropout=0.0, unet_prefix='', net_conv_after_unet=128, net_input_shape=(None, None, 3), net_mask_shape=(None, None, 1), train_shape_completion=False, train_completion_crop=32, train_patch_size=(512, 512), train_background_reg=0.0001, train_foreground_only=0.9, train_sample_cache=True, train_dist_loss='mae', train_loss_weights=(1, 0.2), train_class_weights=(1, 1), train_epochs=400, train_steps_per_epoch=100, train_learning_rate=0.0003, train_batch_size=4, train_n_val_patches=None, train_tensorboard=True, train_reduce_lr={'factor': 0.5, 'patience': 40, 'min_delta': 0}, use_gpu=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_dim': 2,\n",
       " 'axes': 'YXC',\n",
       " 'n_channel_in': 3,\n",
       " 'n_channel_out': 33,\n",
       " 'train_checkpoint': 'weights_best.h5',\n",
       " 'train_checkpoint_last': 'weights_last.h5',\n",
       " 'train_checkpoint_epoch': 'weights_now.h5',\n",
       " 'n_rays': 32,\n",
       " 'grid': (4, 4),\n",
       " 'backbone': 'unet',\n",
       " 'n_classes': None,\n",
       " 'unet_n_depth': 3,\n",
       " 'unet_kernel_size': (3, 3),\n",
       " 'unet_n_filter_base': 32,\n",
       " 'unet_n_conv_per_depth': 2,\n",
       " 'unet_pool': (2, 2),\n",
       " 'unet_activation': 'relu',\n",
       " 'unet_last_activation': 'relu',\n",
       " 'unet_batch_norm': False,\n",
       " 'unet_dropout': 0.0,\n",
       " 'unet_prefix': '',\n",
       " 'net_conv_after_unet': 128,\n",
       " 'net_input_shape': (None, None, 3),\n",
       " 'net_mask_shape': (None, None, 1),\n",
       " 'train_shape_completion': False,\n",
       " 'train_completion_crop': 32,\n",
       " 'train_patch_size': (512, 512),\n",
       " 'train_background_reg': 0.0001,\n",
       " 'train_foreground_only': 0.9,\n",
       " 'train_sample_cache': True,\n",
       " 'train_dist_loss': 'mae',\n",
       " 'train_loss_weights': (1, 0.2),\n",
       " 'train_class_weights': (1, 1),\n",
       " 'train_epochs': 400,\n",
       " 'train_steps_per_epoch': 100,\n",
       " 'train_learning_rate': 0.0003,\n",
       " 'train_batch_size': 4,\n",
       " 'train_n_val_patches': None,\n",
       " 'train_tensorboard': True,\n",
       " 'train_reduce_lr': {'factor': 0.5, 'patience': 40, 'min_delta': 0},\n",
       " 'use_gpu': False}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 32 is a good default choice (see 1_data.ipynb)\n",
    "n_rays = 32\n",
    "\n",
    "# Use OpenCL-based computations for data generator during training (requires 'gputools')\n",
    "use_gpu = True and gputools_available()\n",
    "\n",
    "# Predict on subsampled grid for increased efficiency and larger field of view\n",
    "grid = (4,4)\n",
    "\n",
    "conf = Config2D (\n",
    "    n_rays       = n_rays,\n",
    "    grid         = grid,\n",
    "    use_gpu      = use_gpu,\n",
    "    n_channel_in = n_channel,\n",
    "    train_patch_size = (512,512),\n",
    "    train_steps_per_epoch = 100,\n",
    "    train_epochs = 400,\n",
    ")\n",
    "print(conf)\n",
    "vars(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e50e69c-015f-4784-b78e-df5b98e389bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default values: prob_thresh=0.5, nms_thresh=0.4.\n"
     ]
    }
   ],
   "source": [
    "model = StarDist2D(conf, name='BloodCells', basedir='models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c82c6900-f44e-419d-8864-3e4cb3d1f54c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m median_size \u001b[38;5;241m=\u001b[39m calculate_extents(\u001b[38;5;28mlist\u001b[39m(Y_labels), np\u001b[38;5;241m.\u001b[39mmedian)\n\u001b[0;32m      2\u001b[0m fov \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(model\u001b[38;5;241m.\u001b[39m_axes_tile_overlap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYX\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian object size:      \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmedian_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\stardist\\Lib\\site-packages\\stardist\\utils.py:183\u001b[0m, in \u001b[0;36mcalculate_extents\u001b[1;34m(lbl, func)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Aggregate bounding box sizes of objects in label images. \"\"\"\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(lbl,np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m lbl\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lbl,np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m  \u001b[38;5;28misinstance\u001b[39m(lbl,Iterable)):\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(np\u001b[38;5;241m.\u001b[39mstack([calculate_extents(_lbl,func) \u001b[38;5;28;01mfor\u001b[39;00m _lbl \u001b[38;5;129;01min\u001b[39;00m lbl], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    185\u001b[0m n \u001b[38;5;241m=\u001b[39m lbl\u001b[38;5;241m.\u001b[39mndim\n\u001b[0;32m    186\u001b[0m n \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m _raise(\u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel image should be 2- or 3-dimensional (or pass a list of these)\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\stardist\\Lib\\site-packages\\numpy\\core\\shape_base.py:449\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[0;32m    447\u001b[0m shapes \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall input arrays must have the same shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    451\u001b[0m result_ndim \u001b[38;5;241m=\u001b[39m arrays[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    452\u001b[0m axis \u001b[38;5;241m=\u001b[39m normalize_axis_index(axis, result_ndim)\n",
      "\u001b[1;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "median_size = calculate_extents(list(Y_labels), np.median)\n",
    "fov = np.array(model._axes_tile_overlap('YX'))\n",
    "print(f\"median object size:      {median_size}\")\n",
    "print(f\"network field of view :  {fov}\")\n",
    "if any(median_size > fov):\n",
    "    print(\"WARNING: median object size larger than field of view of the neural network.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a83e6f1-2489-487b-b464-7b9f8f2c8422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_fliprot(img, mask): \n",
    "    assert img.ndim >= mask.ndim\n",
    "    axes = tuple(range(mask.ndim))\n",
    "    perm = tuple(np.random.permutation(axes))\n",
    "    img = img.transpose(perm + tuple(range(mask.ndim, img.ndim))) \n",
    "    mask = mask.transpose(perm) \n",
    "    for ax in axes: \n",
    "        if np.random.rand() > 0.5:\n",
    "            img = np.flip(img, axis=ax)\n",
    "            mask = np.flip(mask, axis=ax)\n",
    "    return img, mask \n",
    "\n",
    "def random_intensity_change(img):\n",
    "    img = img*np.random.uniform(0.6,2) + np.random.uniform(-0.2,0.2)\n",
    "    return img\n",
    "\n",
    "\n",
    "def augmenter(x, y):\n",
    "    \"\"\"Augmentation of a single input/label image pair.\n",
    "    x is an input image\n",
    "    y is the corresponding ground-truth label image\n",
    "    \"\"\"\n",
    "    x, y = random_fliprot(x, y)\n",
    "    x = random_intensity_change(x)\n",
    "    # add some gaussian noise\n",
    "    sig = 0.02*np.random.uniform(0,1)\n",
    "    x = x + sig*np.random.normal(0,1,x.shape)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95d3c59-f6f5-4b30-b509-08c2c5ed7ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(X_trn, Y_trn, validation_data=(X_val,Y_val), augmenter=augmenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce95d8-93a8-492f-9db2-a93fa151eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimize_thresholds(X_val, Y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StarDistGPU",
   "language": "python",
   "name": "stardistgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
